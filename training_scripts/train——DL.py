import paddle
import paddle.nn as nn
import paddle.vision.transforms as T
from paddle.io import DataLoader, Dataset
from paddle.optimizer import AdamW
import os
import numpy as np
import time
import pandas as pd
from PIL import Image

print(f"PaddlePaddleç‰ˆæœ¬: {paddle.__version__}")


# ä¼˜åŒ–é…ç½®å‚æ•°
class Config:
    data_dir = '/home/aistudio/work/flower_dataset'
    num_classes = 100
    image_size = 224
    batch_size = 32
    learning_rate = 1e-4  # ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ çŽ‡
    weight_decay = 1e-4
    num_epochs = 50
    save_dir = '/home/aistudio/work/output_vit_fixed'
    log_interval = 20

    os.makedirs(save_dir, exist_ok=True)


def count_parameters(model):
    """ç»Ÿè®¡æ¨¡åž‹å‚æ•°é‡"""
    total_params = 0
    for param in model.parameters():
        total_params += int(param.numel())
    return total_params


# ä¿®å¤çš„ViTæ¨¡åž‹
class FixedViT(nn.Layer):
    def __init__(self, image_size=224, patch_size=16, num_classes=1000,
                 dim=384, depth=6, heads=8, mlp_ratio=4, dropout=0.1):
        super().__init__()

        self.patch_size = patch_size
        self.num_patches = (image_size // patch_size) ** 2
        patch_dim = 3 * patch_size ** 2

        # PatchåµŒå…¥ - ä½¿ç”¨Conv2Dè€Œä¸æ˜¯Linearï¼Œæ›´ç¨³å®š
        self.patch_embed = nn.Conv2D(
            3, dim,
            kernel_size=patch_size,
            stride=patch_size
        )

        # ç±»åˆ«tokenå’Œä½ç½®ç¼–ç 
        self.cls_token = self.create_parameter(
            shape=[1, 1, dim],
            default_initializer=nn.initializer.TruncatedNormal(std=0.02)
        )
        self.pos_embedding = self.create_parameter(
            shape=[1, self.num_patches + 1, dim],
            default_initializer=nn.initializer.TruncatedNormal(std=0.02)
        )

        self.dropout = nn.Dropout(dropout)

        # Transformerå±‚
        mlp_dim = int(dim * mlp_ratio)
        self.encoder_layers = nn.LayerList([
            TransformerBlock(dim, heads, mlp_dim, dropout)
            for _ in range(depth)
        ])

        # å±‚å½’ä¸€åŒ–
        self.norm = nn.LayerNorm(dim)

        # åˆ†ç±»å¤´
        self.head = nn.Linear(dim, num_classes)

        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            nn.initializer.TruncatedNormal(std=0.02)(m.weight)
            if m.bias is not None:
                nn.initializer.Constant(0)(m.bias)
        elif isinstance(m, nn.LayerNorm):
            nn.initializer.Constant(0)(m.bias)
            nn.initializer.Constant(1.0)(m.weight)
        elif isinstance(m, nn.Conv2D):
            nn.initializer.TruncatedNormal(std=0.02)(m.weight)
            if m.bias is not None:
                nn.initializer.Constant(0)(m.bias)

    def forward(self, x):
        B, C, H, W = x.shape

        # ä½¿ç”¨å·ç§¯è¿›è¡ŒpatchåµŒå…¥
        x = self.patch_embed(x)  # [B, dim, H//P, W//P]
        x = x.flatten(2)  # [B, dim, num_patches]
        x = x.transpose([0, 2, 1])  # [B, num_patches, dim]

        # æ·»åŠ ç±»åˆ«token
        cls_tokens = self.cls_token.expand([B, -1, -1])
        x = paddle.concat([cls_tokens, x], axis=1)

        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pos_embedding
        x = self.dropout(x)

        # Transformerç¼–ç å™¨
        for layer in self.encoder_layers:
            x = layer(x)

        # åˆ†ç±»
        x = self.norm(x)
        x = x[:, 0]  # å–ç±»åˆ«token
        x = self.head(x)

        return x


class TransformerBlock(nn.Layer):
    def __init__(self, dim, heads, mlp_dim, dropout=0.1):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = nn.MultiHeadAttention(dim, heads, dropout=dropout)
        self.norm2 = nn.LayerNorm(dim)
        self.mlp = nn.Sequential(
            nn.Linear(dim, mlp_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_dim, dim),
            nn.Dropout(dropout)
        )
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # è‡ªæ³¨æ„åŠ›
        residual = x
        x = self.norm1(x)
        attn_output = self.attn(x, x, x)
        x = residual + self.dropout(attn_output)

        # MLP
        residual = x
        x = self.norm2(x)
        mlp_output = self.mlp(x)
        x = residual + self.dropout(mlp_output)

        return x


class FlowerDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        self.samples = []
        self.labels = []

        # è¯»å–ç±»åˆ«
        categories = sorted([d for d in os.listdir(data_dir) if d.isdigit()], key=int)
        self.label_mapping = {int(cat): idx for idx, cat in enumerate(categories)}

        print(f"åŠ è½½æ•°æ®é›†: {data_dir}")
        print(f"å‘çŽ°ç±»åˆ«: {len(categories)}ä¸ª")

        for category in categories:
            category_dir = os.path.join(data_dir, category)
            if os.path.isdir(category_dir):
                for file in os.listdir(category_dir):
                    if file.lower().endswith(('.jpg', '.jpeg', '.png')):
                        self.samples.append(os.path.join(category_dir, file))
                        self.labels.append(self.label_mapping[int(category)])

        print(f"å›¾ç‰‡æ•°é‡: {len(self.samples)}")

    def __getitem__(self, idx):
        img_path = self.samples[idx]
        label = self.labels[idx]

        try:
            img = Image.open(img_path).convert('RGB')
            if self.transform:
                img = self.transform(img)
            return img, label
        except Exception as e:
            print(f"åŠ è½½å›¾ç‰‡å¤±è´¥: {img_path}, é”™è¯¯: {e}")
            dummy_img = paddle.zeros([3, Config.image_size, Config.image_size])
            return dummy_img, label

    def __len__(self):
        return len(self.samples)


def create_transforms():
    image_size = Config.image_size

    train_transforms = T.Compose([
        T.Resize((image_size, image_size)),
        T.RandomHorizontalFlip(0.3),
        T.ColorJitter(brightness=0.2, contrast=0.2),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    val_transforms = T.Compose([
        T.Resize((image_size, image_size)),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    return train_transforms, val_transforms


def create_model():
    # ä½¿ç”¨ä¸­ç­‰å¤§å°çš„ViTæ¨¡åž‹
    model = FixedViT(
        image_size=Config.image_size,
        patch_size=16,
        num_classes=Config.num_classes,
        dim=384,  # ä¸­ç­‰ç»´åº¦
        depth=6,  # ä¸­ç­‰æ·±åº¦
        heads=8,  # ä¸­ç­‰å¤´æ•°
        mlp_ratio=4,  # MLPæ‰©å±•æ¯”ä¾‹
        dropout=0.1
    )

    print(f"åˆ›å»ºä¿®å¤çš„ViTæ¨¡åž‹")
    print(f"è¾“å…¥å°ºå¯¸: {Config.image_size}")
    print(f"ç±»åˆ«æ•°: {Config.num_classes}")

    total_params = count_parameters(model)
    print(f"æ€»å‚æ•°é‡: {total_params:,}")

    return model


def test_model_forward():
    """æµ‹è¯•æ¨¡åž‹å‰å‘ä¼ æ’­"""
    print("æµ‹è¯•æ¨¡åž‹å‰å‘ä¼ æ’­...")
    model = create_model()

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_input = paddle.randn([2, 3, Config.image_size, Config.image_size])

    # å‰å‘ä¼ æ’­
    with paddle.no_grad():
        output = model(test_input)

    print(f"è¾“å…¥å½¢çŠ¶: {test_input.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"è¾“å‡ºèŒƒå›´: [{output.min().item():.3f}, {output.max().item():.3f}]")

    # æµ‹è¯•æŸå¤±è®¡ç®—
    criterion = nn.CrossEntropyLoss()
    test_target = paddle.randint(0, Config.num_classes, [2])
    loss = criterion(output, test_target)
    print(f"æµ‹è¯•æŸå¤±: {loss.item():.4f}")

    # æ£€æŸ¥æ¢¯åº¦
    model.train()
    test_output = model(test_input)
    test_loss = criterion(test_output, test_target)
    test_loss.backward()

    # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦å­˜åœ¨
    grad_norms = []
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = paddle.norm(param.grad).item()
            grad_norms.append(grad_norm)

    if grad_norms:
        print(f"æ¢¯åº¦èŒƒæ•°èŒƒå›´: [{min(grad_norms):.6f}, {max(grad_norms):.6f}]")
    else:
        print("âŒ æ²¡æœ‰æ£€æµ‹åˆ°æ¢¯åº¦")

    return model


def train_fixed_vit():
    """è®­ç»ƒä¿®å¤çš„ViTæ¨¡åž‹"""
    print("=" * 60)
    print("å¼€å§‹ä¿®å¤çš„ViTè®­ç»ƒ")
    print("=" * 60)

    # è®¾ç½®éšæœºç§å­
    paddle.seed(42)
    np.random.seed(42)

    # æ•°æ®ç›®å½•
    train_dir = os.path.join(Config.data_dir, 'train')
    val_dir = os.path.join(Config.data_dir, 'val')

    # åˆ›å»ºæ•°æ®å˜æ¢
    train_transforms, val_transforms = create_transforms()

    # åˆ›å»ºæ•°æ®é›†
    print("åŠ è½½è®­ç»ƒé›†...")
    train_dataset = FlowerDataset(train_dir, transform=train_transforms)
    print("åŠ è½½éªŒè¯é›†...")
    val_dataset = FlowerDataset(val_dir, transform=val_transforms)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=Config.batch_size,
        shuffle=True,
        num_workers=2
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=Config.batch_size,
        shuffle=False,
        num_workers=2
    )

    print(f"\næ•°æ®é›†ç»Ÿè®¡:")
    print(f"è®­ç»ƒé›†: {len(train_dataset)} å¼ å›¾ç‰‡")
    print(f"éªŒè¯é›†: {len(val_dataset)} å¼ å›¾ç‰‡")
    print(f"æ‰¹æ¬¡å¤§å°: {Config.batch_size}")

    # æµ‹è¯•æ¨¡åž‹
    model = test_model_forward()

    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = AdamW(
        parameters=model.parameters(),
        learning_rate=Config.learning_rate,
        weight_decay=Config.weight_decay
    )

    criterion = nn.CrossEntropyLoss()

    # è®­ç»ƒè®°å½•
    train_losses = []
    train_accuracies = []
    val_losses = []
    val_accuracies = []
    best_accuracy = 0.0

    print("\nå¼€å§‹è®­ç»ƒ...")
    print("=" * 60)

    for epoch in range(1, Config.num_epochs + 1):
        print(f"\nEpoch {epoch}/{Config.num_epochs}")
        print("-" * 50)

        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_loss = 0
        correct = 0
        total = 0
        start_time = time.time()

        for batch_idx, (data, target) in enumerate(train_loader):
            optimizer.clear_grad()

            output = model(data)
            loss = criterion(output, target)

            loss.backward()
            optimizer.step()

            total_loss += loss.item()

            pred = output.argmax(axis=1)
            correct += (pred == target).sum().item()
            total += target.shape[0]

            if batch_idx % Config.log_interval == 0:
                current_lr = optimizer.get_lr()
                batch_acc = 100. * (pred == target).astype('float32').mean().item()
                print(
                    f'è®­ç»ƒ [{batch_idx}/{len(train_loader)}] Loss: {loss.item():.4f} Acc: {batch_acc:.1f}% LR: {current_lr:.2e}')

        avg_train_loss = total_loss / len(train_loader)
        train_accuracy = 100. * correct / total
        train_time = time.time() - start_time

        train_losses.append(avg_train_loss)
        train_accuracies.append(train_accuracy)

        print(f'è®­ç»ƒç»“æžœ - æŸå¤±: {avg_train_loss:.4f}, å‡†ç¡®çŽ‡: {train_accuracy:.2f}%, æ—¶é—´: {train_time:.1f}ç§’')

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0

        with paddle.no_grad():
            for data, target in val_loader:
                output = model(data)
                loss = criterion(output, target)

                val_loss += loss.item()
                pred = output.argmax(axis=1)
                val_correct += (pred == target).sum().item()
                val_total += target.shape[0]

        avg_val_loss = val_loss / len(val_loader)
        val_accuracy = 100. * val_correct / val_total

        val_losses.append(avg_val_loss)
        val_accuracies.append(val_accuracy)

        print(f'éªŒè¯ç»“æžœ - æŸå¤±: {avg_val_loss:.4f}, å‡†ç¡®çŽ‡: {val_accuracy:.2f}%')

        # ä¿å­˜æœ€ä½³æ¨¡åž‹
        if val_accuracy > best_accuracy:
            best_accuracy = val_accuracy
            best_model_path = os.path.join(Config.save_dir, 'best_model.pdparams')
            paddle.save(model.state_dict(), best_model_path)
            print(f"ðŸš€ ä¿å­˜æœ€ä½³æ¨¡åž‹ï¼Œå‡†ç¡®çŽ‡: {val_accuracy:.2f}%")

        # æ¯5ä¸ªepochä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
        if epoch % 5 == 0:
            checkpoint_path = os.path.join(Config.save_dir, f'checkpoint_epoch_{epoch}.pdparams')
            paddle.save(model.state_dict(), checkpoint_path)
            print(f"ðŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")

        # æå‰åœæ­¢æ£€æŸ¥ - æ”¾å®½æ¡ä»¶
        if epoch >= 5 and best_accuracy < 3.0:
            print("âŒ å‡†ç¡®çŽ‡è¿‡ä½Žï¼Œåœæ­¢è®­ç»ƒ")
            break
        elif epoch >= 5 and avg_train_loss > 4.0 and train_accuracy < 5.0:
            print("âŒ æŸå¤±æ²¡æœ‰ä¸‹é™ï¼Œåœæ­¢è®­ç»ƒ")
            break

    # ä¿å­˜æœ€ç»ˆæ¨¡åž‹
    final_model_path = os.path.join(Config.save_dir, 'final_model.pdparams')
    paddle.save(model.state_dict(), final_model_path)
    print(f"ðŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡åž‹: {final_model_path}")

    # è®­ç»ƒæ€»ç»“
    print("\n" + "=" * 60)
    print("ðŸŽ‰ è®­ç»ƒå®Œæˆ!")
    print(f"ðŸ† æœ€ä½³éªŒè¯å‡†ç¡®çŽ‡: {best_accuracy:.2f}%")
    print("=" * 60)

    # ä¿å­˜è®­ç»ƒåŽ†å²
    history = pd.DataFrame({
        'epoch': range(1, len(train_losses) + 1),
        'train_loss': train_losses,
        'train_accuracy': train_accuracies,
        'val_loss': val_losses,
        'val_accuracy': val_accuracies
    })
    history_path = os.path.join(Config.save_dir, 'training_history.csv')
    history.to_csv(history_path, index=False)
    print(f"ðŸ“ˆ è®­ç»ƒåŽ†å²å·²ä¿å­˜: {history_path}")

    return model, history


if __name__ == "__main__":
    model, history = train_fixed_vit()