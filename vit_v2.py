import paddle
import paddle.nn as nn
import paddle.vision.transforms as T
from paddle.io import DataLoader, Dataset
from paddle.optimizer import AdamW
from paddle.optimizer.lr import CosineAnnealingDecay
import os
import numpy as np
import time
import pandas as pd
from PIL import Image
import math

print(f"PaddlePaddleç‰ˆæœ¬: {paddle.__version__}")


# ä¼˜åŒ–é…ç½®å‚æ•°
class Config:
    data_dir = '/home/aistudio/work/flower_dataset'
    num_classes = 100
    image_size = 224
    batch_size = 32
    learning_rate = 1e-4
    weight_decay = 1e-4
    num_epochs = 100
    save_dir = '/home/aistudio/work/output_vit_improved'
    log_interval = 20
    resume_checkpoint = None

    # è°ƒæ•´æ­£åˆ™åŒ–å‚æ•°
    dropout_rate = 0.1
    stochastic_depth_rate = 0.05

    os.makedirs(save_dir, exist_ok=True)


def count_parameters(model):
    """ç»Ÿè®¡æ¨¡å‹å‚æ•°é‡"""
    total_params = 0
    for param in model.parameters():
        total_params += int(param.numel())
    return total_params


# æ ‡ç­¾å¹³æ»‘äº¤å‰ç†µæŸå¤±
class LabelSmoothCrossEntropyLoss(nn.Layer):
    def __init__(self, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing

    def forward(self, pred, target):
        log_prob = nn.functional.log_softmax(pred, axis=-1)
        nll_loss = -paddle.take_along_axis(log_prob, target.unsqueeze(1), axis=1)
        nll_loss = nll_loss.squeeze(1)

        smooth_loss = -log_prob.mean(axis=-1)
        loss = (1 - self.smoothing) * nll_loss + self.smoothing * smooth_loss
        return loss.mean()


# æ·»åŠ éšæœºæ·±åº¦ï¼ˆStochastic Depthï¼‰
class StochasticDepth(nn.Layer):
    def __init__(self, drop_rate):
        super().__init__()
        self.drop_rate = drop_rate

    def forward(self, x):
        if not self.training or self.drop_rate == 0:
            return x

        keep_prob = 1 - self.drop_rate
        shape = [x.shape[0]] + [1] * (x.ndim - 1)
        random_tensor = paddle.rand(shape, dtype=x.dtype) + keep_prob
        random_tensor = paddle.floor(random_tensor)

        return x / keep_prob * random_tensor


# ä¿®å¤çš„ViTæ¨¡å‹ - ç®€åŒ–æ¶æ„
class ImprovedViT(nn.Layer):
    def __init__(self, image_size=224, patch_size=16, num_classes=1000,
                 dim=384, depth=6, heads=8, mlp_ratio=4, dropout=0.1,
                 stochastic_depth_rate=0.1):
        super().__init__()

        self.patch_size = patch_size
        self.num_patches = (image_size // patch_size) ** 2

        # PatchåµŒå…¥ - ä½¿ç”¨åŸå§‹ViTçš„æ–¹æ³•
        self.patch_embed = nn.Conv2D(
            3, dim,
            kernel_size=patch_size,
            stride=patch_size
        )

        # ç±»åˆ«tokenå’Œä½ç½®ç¼–ç 
        self.cls_token = self.create_parameter(
            shape=[1, 1, dim],
            default_initializer=nn.initializer.TruncatedNormal(std=0.02)
        )
        self.pos_embedding = self.create_parameter(
            shape=[1, self.num_patches + 1, dim],
            default_initializer=nn.initializer.TruncatedNormal(std=0.02)
        )

        self.dropout = nn.Dropout(dropout)

        # Transformerå±‚
        mlp_dim = int(dim * mlp_ratio)
        self.encoder_layers = nn.LayerList([
            TransformerBlock(dim, heads, mlp_dim, dropout,
                             stochastic_depth_rate * (i / (depth - 1)) if depth > 1 else 0)
            for i in range(depth)
        ])

        # å±‚å½’ä¸€åŒ–
        self.norm = nn.LayerNorm(dim)

        # åˆ†ç±»å¤´
        self.head = nn.Linear(dim, num_classes)

        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            nn.initializer.TruncatedNormal(std=0.02)(m.weight)
            if m.bias is not None:
                nn.initializer.Constant(0)(m.bias)
        elif isinstance(m, nn.LayerNorm):
            nn.initializer.Constant(0)(m.bias)
            nn.initializer.Constant(1.0)(m.weight)
        elif isinstance(m, nn.Conv2D):
            nn.initializer.TruncatedNormal(std=0.02)(m.weight)
            if m.bias is not None:
                nn.initializer.Constant(0)(m.bias)

    def forward(self, x):
        B, C, H, W = x.shape

        # ä½¿ç”¨å·ç§¯è¿›è¡ŒpatchåµŒå…¥
        x = self.patch_embed(x)  # [B, dim, H//P, W//P]
        x = x.flatten(2)  # [B, dim, num_patches]
        x = x.transpose([0, 2, 1])  # [B, num_patches, dim]

        # æ·»åŠ ç±»åˆ«token
        cls_tokens = self.cls_token.expand([B, -1, -1])
        x = paddle.concat([cls_tokens, x], axis=1)

        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pos_embedding
        x = self.dropout(x)

        # Transformerç¼–ç å™¨
        for layer in self.encoder_layers:
            x = layer(x)

        # åˆ†ç±»
        x = self.norm(x)
        x = x[:, 0]  # å–ç±»åˆ«token
        x = self.head(x)

        return x


class TransformerBlock(nn.Layer):
    def __init__(self, dim, heads, mlp_dim, dropout=0.1, stochastic_depth_rate=0.0):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = nn.MultiHeadAttention(dim, heads, dropout=dropout)
        self.norm2 = nn.LayerNorm(dim)
        self.mlp = nn.Sequential(
            nn.Linear(dim, mlp_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_dim, dim),
            nn.Dropout(dropout)
        )
        self.stochastic_depth = StochasticDepth(stochastic_depth_rate)

    def forward(self, x):
        # è‡ªæ³¨æ„åŠ› + éšæœºæ·±åº¦
        residual = x
        x = self.norm1(x)
        attn_output = self.attn(x, x, x)
        x = self.stochastic_depth(attn_output)
        x = residual + x

        # MLP + éšæœºæ·±åº¦
        residual = x
        x = self.norm2(x)
        mlp_output = self.mlp(x)
        x = self.stochastic_depth(mlp_output)
        x = residual + x

        return x


class FlowerDataset(Dataset):
    def __init__(self, data_dir, transform=None, is_train=True):
        self.data_dir = data_dir
        self.transform = transform
        self.is_train = is_train
        self.samples = []
        self.labels = []

        # è¯»å–ç±»åˆ«
        categories = sorted([d for d in os.listdir(data_dir) if d.isdigit()], key=int)
        self.label_mapping = {int(cat): idx for idx, cat in enumerate(categories)}

        print(f"åŠ è½½æ•°æ®é›†: {data_dir}")
        print(f"å‘ç°ç±»åˆ«: {len(categories)}ä¸ª")

        for category in categories:
            category_dir = os.path.join(data_dir, category)
            if os.path.isdir(category_dir):
                for file in os.listdir(category_dir):
                    if file.lower().endswith(('.jpg', '.jpeg', '.png')):
                        self.samples.append(os.path.join(category_dir, file))
                        self.labels.append(self.label_mapping[int(category)])

        print(f"å›¾ç‰‡æ•°é‡: {len(self.samples)}")

    def __getitem__(self, idx):
        img_path = self.samples[idx]
        label = self.labels[idx]

        try:
            img = Image.open(img_path).convert('RGB')
            if self.transform:
                img = self.transform(img)
            return img, label
        except Exception as e:
            print(f"åŠ è½½å›¾ç‰‡å¤±è´¥: {img_path}, é”™è¯¯: {e}")
            dummy_img = paddle.rand([3, Config.image_size, Config.image_size])
            return dummy_img, label

    def __len__(self):
        return len(self.samples)


def create_transforms():
    """åˆ›å»ºå¢å¼ºçš„æ•°æ®å˜æ¢"""
    image_size = Config.image_size

    train_transforms = T.Compose([
        T.Resize((image_size + 32, image_size + 32)),
        T.RandomCrop(image_size),
        T.RandomHorizontalFlip(0.5),
        T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

    val_transforms = T.Compose([
        T.Resize((image_size, image_size)),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    return train_transforms, val_transforms


def create_model():
    """åˆ›å»ºæ”¹è¿›çš„ViTæ¨¡å‹"""
    model = ImprovedViT(
        image_size=Config.image_size,
        patch_size=16,
        num_classes=Config.num_classes,
        dim=384,
        depth=6,
        heads=8,
        mlp_ratio=4,
        dropout=Config.dropout_rate,
        stochastic_depth_rate=Config.stochastic_depth_rate
    )

    print(f"åˆ›å»ºæ”¹è¿›çš„ViTæ¨¡å‹")
    print(f"è¾“å…¥å°ºå¯¸: {Config.image_size}")
    print(f"ç±»åˆ«æ•°: {Config.num_classes}")
    print(f"Dropoutç‡: {Config.dropout_rate}")
    print(f"éšæœºæ·±åº¦ç‡: {Config.stochastic_depth_rate}")

    total_params = count_parameters(model)
    print(f"æ€»å‚æ•°é‡: {total_params:,}")

    return model


def create_optimizer_scheduler(model, train_loader):
    """åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
    learning_rate = Config.learning_rate

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = AdamW(
        learning_rate=learning_rate,
        parameters=model.parameters(),
        weight_decay=Config.weight_decay
    )

    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = CosineAnnealingDecay(
        learning_rate=learning_rate,
        T_max=Config.num_epochs * len(train_loader)
    )

    return optimizer, scheduler


def load_checkpoint(model, optimizer=None, scheduler=None):
    """åŠ è½½æ£€æŸ¥ç‚¹ - ä¿®å¤æƒé‡ä¸åŒ¹é…é—®é¢˜"""
    if Config.resume_checkpoint and os.path.exists(Config.resume_checkpoint):
        print(f"åŠ è½½æ£€æŸ¥ç‚¹: {Config.resume_checkpoint}")
        checkpoint = paddle.load(Config.resume_checkpoint)

        # æ£€æŸ¥æ£€æŸ¥ç‚¹ç±»å‹
        if 'model_state_dict' in checkpoint:
            checkpoint_state_dict = checkpoint['model_state_dict']
        else:
            checkpoint_state_dict = checkpoint

        # è·å–å½“å‰æ¨¡å‹çš„çŠ¶æ€å­—å…¸
        model_state_dict = model.state_dict()

        # åˆ›å»ºæ–°çš„çŠ¶æ€å­—å…¸ï¼ŒåªåŠ è½½åŒ¹é…çš„å‚æ•°
        new_state_dict = {}
        loaded_params = 0
        skipped_params = 0

        for param_name in model_state_dict.keys():
            if param_name in checkpoint_state_dict:
                # æ£€æŸ¥å½¢çŠ¶æ˜¯å¦åŒ¹é…
                if model_state_dict[param_name].shape == checkpoint_state_dict[param_name].shape:
                    new_state_dict[param_name] = checkpoint_state_dict[param_name]
                    loaded_params += 1
                else:
                    print(f"è·³è¿‡å‚æ•° {param_name}: å½¢çŠ¶ä¸åŒ¹é…")
                    new_state_dict[param_name] = model_state_dict[param_name]
                    skipped_params += 1
            else:
                print(f"åˆå§‹åŒ–æ–°å‚æ•°: {param_name}")
                new_state_dict[param_name] = model_state_dict[param_name]
                skipped_params += 1

        # åŠ è½½çŠ¶æ€å­—å…¸
        model.set_state_dict(new_state_dict)
        print(f"å‚æ•°åŠ è½½å®Œæˆ: {loaded_params}ä¸ªå‚æ•°å·²åŠ è½½, {skipped_params}ä¸ªå‚æ•°è·³è¿‡/åˆå§‹åŒ–")

        # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
        if optimizer is not None and 'optimizer_state_dict' in checkpoint:
            try:
                optimizer.set_state_dict(checkpoint['optimizer_state_dict'])
                print("ä¼˜åŒ–å™¨çŠ¶æ€å·²åŠ è½½")
            except:
                print("ä¼˜åŒ–å™¨çŠ¶æ€åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ä¼˜åŒ–å™¨çŠ¶æ€")

        # åŠ è½½è°ƒåº¦å™¨çŠ¶æ€
        if scheduler is not None and 'scheduler_state_dict' in checkpoint:
            try:
                scheduler.set_state_dict(checkpoint['scheduler_state_dict'])
                print("è°ƒåº¦å™¨çŠ¶æ€å·²åŠ è½½")
            except:
                print("è°ƒåº¦å™¨çŠ¶æ€åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è°ƒåº¦å™¨çŠ¶æ€")

        # åŠ è½½è®­ç»ƒçŠ¶æ€
        start_epoch = checkpoint.get('epoch', 0) + 1
        best_accuracy = checkpoint.get('best_accuracy', 0.0)
        train_history = checkpoint.get('train_history', {})

        print(f"ä»epoch {start_epoch}ç»§ç»­è®­ç»ƒ, æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.2f}%")

        return start_epoch, best_accuracy, train_history
    else:
        if Config.resume_checkpoint:
            print(f"è­¦å‘Š: æ£€æŸ¥ç‚¹ {Config.resume_checkpoint} ä¸å­˜åœ¨ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
        return 1, 0.0, {}


def save_checkpoint(epoch, model, optimizer, scheduler, best_accuracy, train_history, is_best=False):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict() if optimizer else None,
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'best_accuracy': best_accuracy,
        'train_history': train_history,
        'config': {
            'image_size': Config.image_size,
            'num_classes': Config.num_classes,
            'batch_size': Config.batch_size,
        }
    }

    # ä¿å­˜å¸¸è§„æ£€æŸ¥ç‚¹
    checkpoint_path = os.path.join(Config.save_dir, f'checkpoint_epoch_{epoch}.pdparams')
    paddle.save(checkpoint, checkpoint_path)

    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if is_best:
        best_model_path = os.path.join(Config.save_dir, 'best_model.pdparams')
        paddle.save(checkpoint, best_model_path)
        print(f"ğŸš€ ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œå‡†ç¡®ç‡: {best_accuracy:.2f}%")

    # ä¿å­˜æœ€æ–°æ¨¡å‹
    latest_model_path = os.path.join(Config.save_dir, 'latest_model.pdparams')
    paddle.save(checkpoint, latest_model_path)

    return checkpoint_path


def train_improved_vit():
    """è®­ç»ƒæ”¹è¿›çš„ViTæ¨¡å‹"""
    print("=" * 60)
    print("å¼€å§‹æ”¹è¿›çš„ViTè®­ç»ƒ")
    print("=" * 60)

    # è®¾ç½®éšæœºç§å­
    paddle.seed(42)
    np.random.seed(42)

    # æ•°æ®ç›®å½•
    train_dir = os.path.join(Config.data_dir, 'train')
    val_dir = os.path.join(Config.data_dir, 'val')

    # åˆ›å»ºæ•°æ®å˜æ¢
    train_transforms, val_transforms = create_transforms()

    # åˆ›å»ºæ•°æ®é›†
    print("åŠ è½½è®­ç»ƒé›†...")
    train_dataset = FlowerDataset(train_dir, transform=train_transforms, is_train=True)
    print("åŠ è½½éªŒè¯é›†...")
    val_dataset = FlowerDataset(val_dir, transform=val_transforms, is_train=False)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=Config.batch_size,
        shuffle=True,
        num_workers=2
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=Config.batch_size,
        shuffle=False,
        num_workers=2
    )

    print(f"\næ•°æ®é›†ç»Ÿè®¡:")
    print(f"è®­ç»ƒé›†: {len(train_dataset)} å¼ å›¾ç‰‡")
    print(f"éªŒè¯é›†: {len(val_dataset)} å¼ å›¾ç‰‡")
    print(f"æ‰¹æ¬¡å¤§å°: {Config.batch_size}")

    # åˆ›å»ºæ¨¡å‹
    model = create_model()

    # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer, scheduler = create_optimizer_scheduler(model, train_loader)

    # åŠ è½½æ£€æŸ¥ç‚¹
    start_epoch, best_accuracy, train_history = load_checkpoint(model, optimizer, scheduler)

    # æŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss()

    # è®­ç»ƒè®°å½•
    if not train_history:
        train_history = {
            'train_loss': [],
            'train_accuracy': [],
            'val_loss': [],
            'val_accuracy': [],
            'learning_rates': []
        }

    print("\nå¼€å§‹è®­ç»ƒ...")
    print("=" * 60)

    for epoch in range(start_epoch, Config.num_epochs + 1):
        print(f"\nEpoch {epoch}/{Config.num_epochs}")
        print("-" * 50)

        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_loss = 0
        correct = 0
        total = 0
        start_time = time.time()

        for batch_idx, (data, target) in enumerate(train_loader):
            current_step = (epoch - 1) * len(train_loader) + batch_idx

            optimizer.clear_grad()

            output = model(data)
            loss = criterion(output, target)

            loss.backward()

            optimizer.step()
            scheduler.step()

            total_loss += loss.item()

            # è®¡ç®—å‡†ç¡®ç‡
            pred = output.argmax(axis=1)
            correct += (pred == target).sum().item()
            total += target.shape[0]

            if batch_idx % Config.log_interval == 0:
                current_lr = optimizer.get_lr()
                batch_acc = 100. * (pred == target).astype('float32').mean().item()
                print(
                    f'è®­ç»ƒ [{batch_idx}/{len(train_loader)}] Loss: {loss.item():.4f} Acc: {batch_acc:.1f}% LR: {current_lr:.2e}')

        avg_train_loss = total_loss / len(train_loader)
        train_accuracy = 100. * correct / total
        train_time = time.time() - start_time

        train_history['train_loss'].append(avg_train_loss)
        train_history['train_accuracy'].append(train_accuracy)
        train_history['learning_rates'].append(optimizer.get_lr())

        print(f'è®­ç»ƒç»“æœ - æŸå¤±: {avg_train_loss:.4f}, å‡†ç¡®ç‡: {train_accuracy:.2f}%, æ—¶é—´: {train_time:.1f}ç§’')

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0

        with paddle.no_grad():
            for data, target in val_loader:
                output = model(data)
                loss = criterion(output, target)

                val_loss += loss.item()
                pred = output.argmax(axis=1)
                val_correct += (pred == target).sum().item()
                val_total += target.shape[0]

        avg_val_loss = val_loss / len(val_loader)
        val_accuracy = 100. * val_correct / val_total

        train_history['val_loss'].append(avg_val_loss)
        train_history['val_accuracy'].append(val_accuracy)

        print(f'éªŒè¯ç»“æœ - æŸå¤±: {avg_val_loss:.4f}, å‡†ç¡®ç‡: {val_accuracy:.2f}%')

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        is_best = val_accuracy > best_accuracy
        if is_best:
            best_accuracy = val_accuracy

        # ä¿å­˜æ£€æŸ¥ç‚¹
        checkpoint_path = save_checkpoint(
            epoch, model, optimizer, scheduler, best_accuracy, train_history, is_best
        )

        if epoch % 5 == 0:
            print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")

        # æå‰åœæ­¢æ£€æŸ¥
        if epoch >= 10 and best_accuracy < 10.0:
            print("âŒ å‡†ç¡®ç‡è¿‡ä½ï¼Œåœæ­¢è®­ç»ƒ")
            break

        if epoch >= 15 and train_accuracy - val_accuracy > 40.0:
            print("âŒ è¿‡æ‹Ÿåˆä¸¥é‡ï¼Œåœæ­¢è®­ç»ƒ")
            break

    # è®­ç»ƒæ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ† æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_accuracy:.2f}%")
    print("=" * 60)

    # ä¿å­˜è®­ç»ƒå†å²
    history_df = pd.DataFrame({
        'epoch': range(1, len(train_history['train_loss']) + 1),
        'train_loss': train_history['train_loss'],
        'train_accuracy': train_history['train_accuracy'],
        'val_loss': train_history['val_loss'],
        'val_accuracy': train_history['val_accuracy'],
        'learning_rate': train_history['learning_rates']
    })
    history_path = os.path.join(Config.save_dir, 'training_history.csv')
    history_df.to_csv(history_path, index=False)
    print(f"ğŸ“ˆ è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")

    return model, history_df


if __name__ == "__main__":
    # è®¾ç½®æ£€æŸ¥ç‚¹è·¯å¾„
    Config.resume_checkpoint = '/home/aistudio/work/best_model.pdparams'
    model, history = train_improved_vit()